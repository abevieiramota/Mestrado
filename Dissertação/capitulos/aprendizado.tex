\chapter{Aprendizado de máquina}

% Definição
\section{Definição}

Aprendizado de máquina é uma subárea de Inteligência Artificial que agrupa conhecimentos sobre algoritmos e técnicas que permitam que um programa melhore sua performance a partir de dados. Mais formalmente:

\begin{citacao}
Um programa aprende a partir de uma experiência E, com relação a uma classe de tarefas T e a uma medida de performance P, se sua performance em tarefas da classe T, medida por P, melhora com a experiência E. \cite[p.2, tradução nossa]{Tom_mitchell}
\end{citacao}

\todo[inline]{adicionar exemplos de aplicação}

% Exemplo
\section{Modelagem}

Para exemplificar a modelagem de um programa com uma abordagem de aprendizado de máquina, \cite{Tom_mitchell} apresenta uma sequência de passos para desenvolver um programa que aprenda a jogar xadrez, a ser utilizado para disputar um campeonato mundial de xadrez, tendo sua performance medida pela frequência de partidas que vencer.

% Escolha da experiência
\subsection*{Escolha da experiência}

O primeiro passo consiste na escolha da experiência a partir da qual o programa irá aprender, denominada experiência de treinamento. 
% exemplos de experiência <- definir
\cite{Tom_mitchell} apresenta três atributos da experiência de treinamento que devem ser levados em consideração por impactarem no sucesso do aprendizado: feedback, nível de controle sobre os exemplos de treinamento e quão bem a a distribuição dos exemplos de treinamento representam a distribuição dos exemplos sobre os quais a performance final do programa será mensurada.

O atributo feedback representa quão direta é a informação fornecida pela experiência para o problema em questão, podendo assumir os valores feedback direto e feedback indireto. Por exemplo, a tupla estado do tabuleiro e o melhor movimento possível a partir desse estado é classificada como experiência de feedback direto: o programa irá atuar realizando movimentos e esse tipo de experiência informa diretamente qual o melhor movimento a ser executado. Já a tupla sequência de movimentos de uma partida e seu resultado final é classificada como experiência de feedback indireto: o resultado final da partida não fornece informação direta sobre a influência dos movimentos que nela foram executados. A atividade de determinar o grau de influência que elementos de uma experiência de feedback indireto têm sobre o resultado é denominada credit assignment. \todo{é necessário traduzir credit assignment?}

Com relação ao nível de controle sobre os exemplos de treinamento, a experiência pode ser classificado como experiência selecionada por especialista, experiência sugerida pelo programa e analisada por um especialista e experiência selecionada e analisada pelo programa. Por exemplo, a experiência será do tipo selecionada por especialista se foi selecionada por um jogador experiente de xadrez, que selecionou estados de tabuleiro e indicou que melhores movimentos poderiam ser feitos, a partir desses estados; será do tipo sugerida pelo programa e analisada por um especialista se o próprio programa selecionar estados de tabuleiro para serem analisadas por um jogador experiente de xadrez; será do tipo selecionada e analisada pelo programa se o programa utilizar o resultado de partidas que disputar consigo mesmo.

Com relação a quão bem a a distribuição dos exemplos de treinamento representam a distribuição dos exemplos sobre os quais a performance final do programa será mensurada, a experiência pode ser classificado como representativa, se a distribuição de seus exemplos representar a distribuição dos exemplos com os quais o programa efetivamente será utilizado, e não representativo, caso contrário. Por exemplo, a experiência é não representativa caso esteja limitada ao conjunto de partidas de apenas um jogador: considerando que o programa será utilizado em um campeonato mundial, do qual participam jogadores diversos, com estilos de jogo diversos, é esperado que o programa, treinado com essa experiência, depare com estados de tabuleiro que não encontrou no treinamento. \cite{Tom_mitchell} ressalta que muito da teoria de aprendizado de máquina depende da assunção de que a experiência utilizada no treinamento é representativa.

Para o problema em análise, \cite{Tom_mitchell} escolhe como experiência de treinamento um conjunto de partidas jogadas pelo programa contra ele mesmo.

% Escolha da função alvo/target function
\subsection*{Escolha da função alvo}

O próximo passo é a escolha do tipo de conhecimento que deverá ser aprendido, representado por uma função denominada função alvo, e como ele será utilizado pelo programa. Considerando que o programa irá atuar como um jogador de xadrez, uma possível função a ser considerada é uma cujo domínio seja o conjunto de estados de tabuleiro e que retorne o melhor movimento a partir do estado de tabuleiro informado. Esse tipo de conhecimento depende da assunção de que, dado um estado de tabuleiro, existe um melhor movimento a ser executado. O problema de aprendizado dessa função depende, portanto, do problema de determinar quão um movimento influencia no resultado final de uma partida. \todo[inline]{discutir}
\todo[inline]{qual a escolhida no Mitchell? Lembrar de apresentar o exemplo}

Outro exemplo de função alvo, a ser utilizada no exemplo, é uma que tenha como domínio o conjunto de estados de tabuleiro e retorne um número real, indicando quão bom o estado de tabuleiro informado é. O programa irá jogar verificando qual estado de tabuleiro maximiza o valor da função, considerando o conjunto de estados de tabuleiro que podem ser alcançados a partir do estado atual do tabuleiro e de todas jogadas válidas.

% Escolha de uma representação para a função alvo
\subsection*{Escolha de uma representação para a função alvo}

Após a escolha do tipo de conhecimento que deverá ser aprendido, é necessário definir como esse conhecimento será representado. Por exemplo, a função que associa um estado de tabuleiro a um número real pode assumir diversas formas: pode ser uma matriz contendo uma célula com um número real para cada estado de tabuleiro possível; um conjunto de regras que associe atributos do estado do tabuleiro a números reais; uma função polinomial de atributos do estado do tabuleiro em números reais etc. Para dar continuidade ao detalhamento dos passos, \cite{Tom_mitchell} escolhe a seguinte representação de função, denominada $V$:

$V(t) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + w_5 x_5 + w_6 x_6$

Os coeficientes $w_0$ a $w_6$ são parâmetros do programa, a serem ajustados no aprendizado. As variávels $x_1$ a $x_6$ possuem as seguintes definições:

\begin{itemize}
\item $x_1$: número de peças pretas no tabuleiro
\item $x_2$: número de peças brancas no tabuleiro
\item $x_3$: número de reis pretos no tabuleiro
\item $x_4$: número de reis brancos no tabuleiro
\item $x_5$: número de peças pretas ameaçadas por peças brancas no tabuleiro
\item $x_6$: número de peças brancas ameaçadas por peças pretas no tabuleiro
\end{itemize}

Resumindo os passos até aqui realizados, temos:

\begin{itemize}

\item Tarefa: jogar xadrez
\item Medida de performance: frequência de partidas do campeonato mundial de xadrez ganhas
\item Experiência de treinamento: partidas disputadas pelo programa contra ele mesmo
\item Função alvo: $V:Estados\ De\ Tabuleiro \rightarrow \mathbb{R}$
\item Representação da função alvo: $\overset{-}{V}(t) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + w_5 x_5 + w_6 x_6$

\end{itemize}

% Escolha de um algoritmo de aproximação 
\subsection*{Escolha de um algoritmo de aproximação}

O próximo passo consiste na escolha de um algoritmo que irá realizar o treinamento da função, isto é, que, a partir da experiência de treinamento, irá ajustar seus coeficientes a fim de aproximá-la da função alvo. Para tanto, é necessário um conjunto de exemplos de treinamento composto por tuplas, $\langle b, V_{treino}(b) \rangle $, de estado de tabuleiro e o valor da função alvo para esse estado, respectivamente, $b$ e $V_{treino}(b)$. Por exemplo, o seguinte exemplo representa um estado de tabuleiro onde o jogador com peças pretas ganhou:

$\langle \langle x_1 = 3, x_2 = 0, x_3 = 1, x_4 = 0, x_5 = 0, x_6 = 0 \rangle, +100\rangle$

Para o desenvolvimento dos exemplos de treinamento, é necessário estimar um valor para cada estado de tabuleiro alcançado nas partidas disputadas pelo programa contra ele mesmo(experiência de treinamento escolhida), indicando quão bom aquele estado de tabuleiro é. A abordagem adotada no exemplo consiste em atribuir a $V_{treino}(b)$, caso seja um estado final, um valor arbitrário, caso seja um estado intermediário, o valor da função estimada, $\overset{-}{V}(t)$, aplicado ao próximo estado do tabuleiro para o jogador ativo em $b$, $Sucessor(b)$:

\begin{equation}
V_{treino}(b) \leftarrow \overset{-}{V}(t)(Sucessor(b))
\label{eq:critico}
\end{equation}

\cite{Tom_mitchell} indica que, apesar de parecer estranho atribuir aos exemplos de treinamento valores estimados pela função que está sendo treinada, sob certas condições, pode ser provado que essa abordagem converge para uma estimativa perfeita de $V_{treino}$.\todo[inline]{ponto muito confuso do capítulo! qual o valor inicial dos coeficientes? random? qual o valor de V\_treino para o último estado do tabuleiro? usa um valor arbitrário? qual? acho que +100 win -100 loss e intuitivamente parece mesmo que a tendência é convergir}

Para estimar a função alvo a partir dos dados de treinamento é necessário estimar seus pesos, $w_0$ a $w_6$. Para tanto, primeiro é necessário definir uma medida de quão bem a função estimada se ajusta aos exemplos de treinamento. Uma medida comum é o erro quadrático, assim definido:

$E = \sum\limits_{\langle b, V_{treino}(b) \rangle \in \ dados\ de\ treinamento} {(V_{treino}(b) - V(b))}^2$

O problema de estimar a função alvo pode ser modelado então como o problema de encontrar os pesos $w_0$ a $w_6$ que minimizem o erro quadrático sobre os dados de treinamento. Um dos algoritmos que incrementalmente ajusta os pesos aos dados de treinamento, minimizando o erro quadrático, é o Least Mean Square. Esse algoritmo funciona atualizando iterativamente, para cada exemplo de treinamento, os pesos da função, ajustando-os com um valor proporcional a $(V_{treino}(b) - V(b))$: se o valor da função aplicado ao exemplo de treinamento, $V(b)$, for igual ao valor do exemplo de treinamento, $V_{treino}(b)$, o ajuste será nulo; caso seja maior, o ajuste será negativo, fazendo com que, para o dado de treinamento em questão, o valor da função diminua; caso seja menor, o incremento será positivo, fazendo com que, para o dado de treinamento em questão, o valor da função aumente.

\todo[inline]{continuar a discussão dessa parte}

% Conclusão
\subsection*{Conclusão}

Nessa sequência de passos podem ser identificados quatro elementos de um programa que aprende:

\begin{itemize}
\item Sistema de performance: elemento responsável pela utilização do conhecimento aprendido para resolver uma tarefa. No exemplo, será responsável por determinar qual a próxima jogada, dado um estado de tabuleiro, utilizando a função que foi aprendida. 
\item Crítico: elemento responsável por receber como entrada um dado de treinamento e informar a que valor deve ser associado. No exemplo, o crítico é representado pela equação \ref{eq:critico}.
\item Generalizador: elemento responsável por receber como entrada um conjunto de dados de treinamento e retornar uma função estimativa de uma função alvo. No exemplo, o generalizador é o Método dos Mínimos Quadrados.
\item Representação da função alvo: elemento que define a estrutura da função que será utilizada como estimativa da função alvo. No exemplo, foi utilizada como representação uma combinação linear.
\end{itemize}

Outras configurações para esses elementos foram desenvolvidas. Por exemplo, como representação da função alvo pode-se utilizar um grafo em estrutura de árvore, denominado árvore de decisão. Cada nó seu que não seja folha possui uma regra que associa um dado a um de seus nós filhos. Os nós folhas são associados a um valor. Seu funcionamento consiste em apresentar um dado à regra de um nó, inicialmente o nó raiz, e recursivamente aplicar esse procedimento ao nó filho ao qual a regra associa o dado, até que seja alcançado um nó folha, cujo resultado associado é então retornado como o valor da função.

\section{Modelos}
\todo[inline]{definir o que quer dizer por modelo}
representation + evaluation + optimization \cite{ML_know}

% [Aplicações]
\section{Aplicações}

\todo[inline]{melhorar}
De acordo com \cite{Mitchell_discipline}, o conhecimento sobre aprendizado de máquina pode ser aplicado a diversas áreas, como, por exemplo, a reconhecimento de voz; a visão computacional, sendo utilizado no desenvolvimento de sistemas de reconhecimento facial; a controle de robôs.